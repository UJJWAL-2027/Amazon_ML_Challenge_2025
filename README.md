#  PriceIQâ€“ A Multimodal AI Solution for Product Price Prediction

This is basically the soultion of the hackathon - Amazon ML Challenge 2025

This repository contains the solution for the Amazon ML Challenge 2025, focusing on predicting product prices using a multi-modal hybrid ensemble model.


**Note on Group Project & Contributions**
 This was a collaborative group project where all team members contributed to and reviewed the complete codebase. My specific areas of focus were **Image Processing and Feature Engineering** (extracting visual features) and the implementation and tuning of the **LightGBM model**.

## 1. Methodology Overview

Our approach focused on robust multi-modal learning by integrating structured, textual, and visual data. We developed a hybrid ensemble framework combining the strengths of transformer-based NLP models and gradient boosting algorithms.

**Key highlights:**
* Feature-rich modeling from text and images
* K-Fold Cross-Validation for reliable generalization
* Hyperparameter tuning with Optuna
* Weighted blending of model outputs for final predictions

---

## 2. Model Architecture

Our final solution is a three-model ensemble, each leveraging a distinct modality and learning paradigm:

### DistilBERT (Transformer for Text)
* **Goal:** Predict log-transformed prices from `catalog_content`
* **Model:** `distilbert-base-uncased` + custom regression head
* **Training:** Fine-tuned using LoRA (Low-Rank Adaptation)
* **Strength:** Captures deep semantic relationships in product text

### LightGBM
* **Goal:** Model complex non-linear interactions across structured + embedding features
* **Training:** 5-Fold CV with Optuna-tuned hyperparameters
* **Strength:** Fast, interpretable, and robust for tabular + text/image embeddings

### XGBoost
* **Goal:** Complement LightGBM with diverse boosting approach
* **Training:** 5-Fold CV with tuned parameters
* **Strength:** Enhances ensemble robustness and reduces overfitting

**Final Ensemble:** Weighted blend of DistilBERT, LightGBM, and XGBoost predictions.

---

## 3. Feature Engineering

We engineered features from both text and image data sources to maximize predictive power.

### Textual Features
* **Structured Parsing:** Extracted fields such as `item_name`, `value`, `unit`, and `bullet_points`
* **Derived Numeric Features:**
    * `pack_size` (e.g., "Pack of 6")
    * `text length`, `description_length`, `bullet point_count`
    * `price_per_unit` (price/value ratio)
* **Categorical Encoding:** Cleaned and encoded `brand`, `unit`, and binary flags (`is_gluten_free`, `is_organic`, etc.)
* **Semantic Embeddings:** Generated 384-dim BERT embeddings using `sentence-transformers/all-MiniLM-L6-v2`

### Visual Features
* Extracted EfficientNetB0 embeddings from product images (penultimate CNN layer output)
* Represented each image as a fixed-length vector capturing visual context

---

## 4. Getting Started

### Prerequisites

The solution notebook `AmazonMLChallenge_notebook.ipynb` was designed to run in a Google Colab environment with a T4 GPU.

The main libraries required are:
* `pandas`
* `numpy`
* `scikit-learn`
* `lightgbm`
* `xgboost`
* `optuna`
* `torch`
* `transformers` (for DistilBERT)
* `sentence-transformers` (for embeddings)
* `tqdm`
* `gc` (garbage collection)

### Running the Solution

1.  **Environment:** Open `AmazonMLChallenge_notebook.ipynb` in a Jupyter environment like Google Colab, Kaggle Notebooks, or a local VS Code instance.
2.  **Data:** Ensure the training and test datasets are available in the correct path as specified in the notebook.
3.  **Execution:** Run the cells sequentially to perform:
    * Data loading and preprocessing
    * Feature engineering
    * Model training (LGBM, XGBoost, and DistilBERT)
    * Ensembling and prediction
    * Generation of the final submission file.

---

## 5. File Structure

* `AmazonMLChallenge_notebook.ipynb`: The main Jupyter Notebook containing all code for preprocessing, feature engineering, training, and ensembling.
* `Amazon_csv_file.csv`: The final submission file generated by the notebook.
* `AmazonMLChallenge_Readme.pdf`: The original PDF report detailing the project methodology.
* `*.jpeg`: Supporting images (e.g., screenshots of results, code snippets, or model architecture).

---

## 6. Additional Highlights

* **Target Transformation:** The model was trained on `log1p(price)` to handle the skewed distribution of prices and inverse-transformed for final predictions.
* **Memory Optimization:** Employed techniques like downcasting numeric types, converting string columns to categorical, and using `gc.collect()` to manage memory efficiently.
* **Evaluation:** The primary competition metric was **SMAPE** (Symmetric Mean Absolute Percentage Error).
